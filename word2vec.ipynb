{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_json(\"./arxiv-metadata-oai-snapshot.json\")\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepping Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df = df.sort_values(by=\"year\")\n",
    "abstracts = df['summary'].str.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cleaning Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def deep_clean(document):\n",
    "    texts = [token.lemma_ for token in document if not token.is_stop]\n",
    "    if len(texts) > 2:\n",
    "        return ' '.join(texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prep_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['summary'])\n",
    "cleaned_text = [deep_clean(doc) for doc in nlp.pipe(prep_cleaning, batch_size=5000, n_threads=-1)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_df = pd.DataFrame({'clean': cleaned_text})\n",
    "clean_df = clean_df.dropna().drop_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "process_phrases = [row.split() for row in clean_df['clean']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "phrases = Phrases(process_phrases, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentences = bigram[process_phrases]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word Frequencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frequencies = defaultdict(int)\n",
    "for each in sentences:\n",
    "    for i in each:\n",
    "        frequencies[i] += 1\n",
    "len(frequencies)\n",
    "sorted(frequencies, key=frequencies.get, reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "training_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_model.build_vocab(sentences, progress_per=10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_model.train(sentences, total_examples=training_model.corpus_count, epochs=30, report_delay=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_model.init_sims(replace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_model.wv.most_similar(positive=[\"software\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_model.wv.similarity(\"technology\", 'era')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
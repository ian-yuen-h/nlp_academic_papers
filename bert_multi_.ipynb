{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def import_data():\n",
    "    with open('./arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for each_line in f:\n",
    "            yield each_line\n",
    "\n",
    "imported = import_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict_tags = {\"abstract\":[], \"categories\":[]}\n",
    "for each_paper in imported:\n",
    "    parsed = json.loads(each_paper)\n",
    "    abstract= parsed['abstract']\n",
    "    dict_tags[\"abstract\"].append(abstract)\n",
    "    dict_tags[\"categories\"].append(parsed['categories'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame.from_records(dict_tags)\n",
    "df = df.sample(n=100000, random_state=33)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generating Arrays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categories = dict_tags['categories'].apply(lambda x: x.split(' ')).explode().unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#2-way identification\n",
    "dict_label2int = {}\n",
    "for i, key in enumerate(categories):\n",
    "    dict_label2int[key] = i\n",
    "\n",
    "dict_int2label = {}\n",
    "for key, val in dict_label2int.items():\n",
    "    dict_int2label[val] = key"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gen_array(label):\n",
    "    result = np.zeros(len(dict_label2int))\n",
    "    labels = label.split(' ')\n",
    "    for each in labels:\n",
    "        result[dict_label2int[each]] = 1\n",
    "    return np.expand_dims(result, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = [gen_array(tag) for tag in dict_tags[\"categories\"]]\n",
    "labe_array = np.concatenate(labels, axis = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stops = stopwords.words('english')\n",
    "\n",
    "#pre-cleaning\n",
    "cleaned = dict_tags['abstracts'].apply(lambda x : x.lower())\n",
    "cleaned = cleaned.apply(lambda x: x.split(' '))\n",
    "cleaned = cleaned.apply(lambda x: [item for item in x if item not in stops])\n",
    "cleaned = cleaned.apply(lambda x: ' '.join(x))\n",
    "cleaned = cleaned.apply(lambda x: re.sub('[^A-Za-z\\s]+', ' ', x))\n",
    "cleaned = cleaned.apply(lambda x: re.sub('\\n', ' ', x))\n",
    "cleaned = cleaned.apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "cleaned = cleaned.apply(lambda x: re.sub(r'^\\s', '', x))\n",
    "cleaned = cleaned.apply(lambda x: re.sub(r'\\s$', '', x))\n",
    "\n",
    "cleaned = list(cleaned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "text_tokens = tokenizer.batch_encode_plus(cleaned, pad_to_max_length=True, max_length=250, return_tensors='pt')\n",
    "text_tokens['input_ids'].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train/Test Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random.seed(27)\n",
    "samples = random.sample(range(text_tokens['input_ids'].shape[0]), 100000)#text_tokens[\"input_ids\"].shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(text_tokens[\"input_ids\"][sampls,:], dict_tags[samples, :], test_size = 0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, abstract, category):\n",
    "        self.abstracts = abstract\n",
    "        self.categories = category\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.categories.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.abstracts[index, :]\n",
    "        y = self.categories[index, :]\n",
    "        return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#initialize\n",
    "train_data = dataset(x_train, y_train)\n",
    "train_gen = torch.utils.data.DataLoader(train_data, batch_size=128)\n",
    "test_data = dataset(x_test, y_test)\n",
    "test_gen = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training BERT Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\",\n",
    "                                                                    output_hidden_states=True)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.dense_1 = nn.Linear(768, 384)\n",
    "        self.dense_2 = nn.Linear(384, 176)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        hidden_states = self.encoder(tokens)[1][-1][:, 0]\n",
    "        x = F.relu(self.dense_1(hidden_states))\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "model = BERT()\n",
    "model = model.cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for toks, _ in train_gen:\n",
    "    print(model(toks.cuda()).shape)\n",
    "    break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0006)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_training = []\n",
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for data in train_gen:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        #Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        #BCE with logits\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        #Backprop, optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        del inputs\n",
    "        del labels\n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss_training.append(running_loss / num_batches)\n",
    "\n",
    "print(loss_training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#saving ..\n",
    "torch.save(model.state_dict(), 'result.pt')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}